{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from PySpice.Spice.Parser import SpiceParser\n",
    "from PySpice.Spice import BasicElement\n",
    "from PySpice.Spice.Netlist import Node\n",
    "import helpers as h\n",
    "import multiprocessing\n",
    "from tqdm.notebook import tqdm\n",
    "import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.model_selection import GridSearchCV,StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import multiprocessing\n",
    "from sklearn.utils import shuffle\n",
    "import dgsd\n",
    "import pickle\n",
    "import netlsd\n",
    "from sklearn.metrics import f1_score, accuracy_score,roc_auc_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graphs(path):\n",
    "    files =  [path+f for f in listdir(path) if isfile(join(path, f))]\n",
    "    # print(files)\n",
    "    netlists = ( (f, open(f, 'rb').read().decode('utf-8', 'ignore')) for f in files)\n",
    "    valid_files = [ (f,src) for (f, src) in netlists if h.is_valid_netlist(src)]\n",
    "   \n",
    "    train_data, test_data = {},{}\n",
    "    train_ratio = 0.8\n",
    "    train_count = int(len(valid_files)*train_ratio)\n",
    "    print(train_count)\n",
    "    ## splitting train and test data\n",
    "    index = 0\n",
    "    for (f,src) in valid_files:\n",
    "            # print(f)\n",
    "        component_list, g = h.netlist_as_graph(src)\n",
    "        if index<train_count:\n",
    "            train_data[index] = (component_list, g)\n",
    "            index +=1\n",
    "        else:\n",
    "            test_data[index] = (component_list, g)\n",
    "            index +=1\n",
    "    return train_data, test_data\n",
    "def create_datasets_original_circuits(train_data, test_data,min_limit, max_limit):\n",
    "    map_file = 'data/ltspice_examples_label_mapping.pkl'\n",
    "    with open(map_file, 'rb') as f:\n",
    "        mapping = pickle.load(f)\n",
    "        f.close()\n",
    "    train_dataset, test_dataset = [],[]\n",
    "    mapping_keys = list(mapping.keys())\n",
    "    components = []\n",
    "    for ind, (c,g) in train_data.items():\n",
    "        if (g.number_of_nodes()<min_limit) or (g.number_of_nodes()>max_limit):\n",
    "        #         print(\"continue with \",g.number_of_nodes())\n",
    "            continue\n",
    "        types = [type(comp) for comp in c]\n",
    "        flag = False\n",
    "        for t in types:\n",
    "            if t not in mapping_keys:\n",
    "                flag = True\n",
    "                break\n",
    "        if flag ==True:\n",
    "            continue\n",
    "        for t in types:\n",
    "            if t not in components:\n",
    "                components.append(t)\n",
    "        type_mapping = {}\n",
    "        for i, n in enumerate(g.nodes()):\n",
    "            type_mapping[n] = types[i]\n",
    "        nx.set_node_attributes(g, type_mapping, \"type\")\n",
    "        train_dataset.append(g)\n",
    "    for ind, (c,g) in test_data.items():\n",
    "        if (g.number_of_nodes()<min_limit) or (g.number_of_nodes()>max_limit):\n",
    "        #         print(\"continue with \",g.number_of_nodes())\n",
    "            continue\n",
    "        types = [type(comp) for comp in c]\n",
    "        flag = False\n",
    "        for t in types:\n",
    "            if t not in mapping_keys:\n",
    "                flag = True\n",
    "                break\n",
    "        if flag ==True:\n",
    "            continue\n",
    "        for t in types:\n",
    "            if t not in components:\n",
    "                components.append(t)\n",
    "        if len(types)<g.number_of_nodes():\n",
    "            continue\n",
    "        type_mapping = {}\n",
    "        for i, n in enumerate(g.nodes()):\n",
    "                type_mapping[n] = types[i]\n",
    "        nx.set_node_attributes(g, type_mapping, \"type\")\n",
    "        test_dataset.append(g)\n",
    "    mapping_labels = {}\n",
    "    for i, c in enumerate(components):\n",
    "        mapping_labels[c] = i\n",
    "    return train_dataset, test_dataset,mapping_labels\n",
    "            \n",
    "def construct_dataset(train_data, test_data,min_limit, max_limit):\n",
    "    train_dataset, test_dataset = {},{}       \n",
    "    for ind, (c,g) in train_data.items():\n",
    "        if (g.number_of_nodes()<min_limit) or (g.number_of_nodes()>max_limit):\n",
    "    #         print(\"continue with \",g.number_of_nodes())\n",
    "            continue\n",
    "        types = [type(comp) for comp in c]\n",
    "        type_mapping = {}\n",
    "        for i, n in enumerate(g.nodes()):\n",
    "            type_mapping[n] = types[i]\n",
    "        nx.set_node_attributes(g, type_mapping, \"type\")\n",
    "        unique_types= list(set(types))\n",
    "        for i in range(len(unique_types)):\n",
    "            g_ = g.copy()\n",
    "            t = unique_types[i]\n",
    "            for index,cc in enumerate(c):\n",
    "                if type(cc)==t:\n",
    "                    g_.remove_node(index)\n",
    "                    break\n",
    "\n",
    "\n",
    "            if t in list(train_dataset.keys()):\n",
    "                train_dataset[t].append(g_)\n",
    "            else:\n",
    "                train_dataset[t] = [g_]\n",
    "\n",
    "    for ind, (c,g) in test_data.items():\n",
    "        if (g.number_of_nodes()<min_limit) or (g.number_of_nodes()>max_limit):\n",
    "            continue\n",
    "        types = [type(comp) for comp in c]\n",
    "        type_mapping = {}\n",
    "        for i, n in enumerate(g.nodes()):\n",
    "            type_mapping[n] = types[i]\n",
    "        nx.set_node_attributes(g, type_mapping, \"type\")\n",
    "        unique_types= list(set(types))\n",
    "        for i in range(len(unique_types)):\n",
    "            g_ = g.copy()\n",
    "            t = unique_types[i]\n",
    "            for index,cc in enumerate(c):\n",
    "                if type(cc)==t:\n",
    "                    g_.remove_node(index)\n",
    "                    break\n",
    "\n",
    "            if t in list(test_dataset.keys()):\n",
    "                test_dataset[t].append(g_)\n",
    "            else:\n",
    "                test_dataset[t] = [g_]\n",
    "    return train_dataset, test_dataset\n",
    "def filter_data(train_dataset, test_dataset, instances_limit):\n",
    "    train_dataset_filter, test_dataset_filter = {},{}\n",
    "    i = 0\n",
    "    for k, v in train_dataset.items():\n",
    "        if len(v)>instances_limit:\n",
    "#             print(k)\n",
    "            train_dataset_filter[k] = v\n",
    "            test_dataset_filter[k] = test_dataset.get(k)\n",
    "        i +=1\n",
    "    return train_dataset_filter, test_dataset_filter\n",
    "def map_labels(train_dataset_filter):\n",
    "    label_mapping = {}\n",
    "    l = 0\n",
    "    for k, v in train_dataset_filter.items():\n",
    "        label_mapping[k] = l\n",
    "        l +=1\n",
    "    return label_mapping\n",
    "\n",
    "def print_train_test_type_count(train_dataset_filter,test_dataset_filter):\n",
    "    counter_train = count_test = 0\n",
    "    for k, v in train_dataset_filter.items():\n",
    "        print(\"train:\",k, len(v))\n",
    "        print(\"test:\",len(test_dataset_filter.get(k)))\n",
    "        counter_train +=len(v)\n",
    "        count_test +=len(test_dataset_filter.get(k))\n",
    "    print(counter_train, count_test)\n",
    "def write_mapping(file, mapping):\n",
    "    with open(file,\"wb\") as f:\n",
    "        pickle.dump(mapping,f)\n",
    "    f.close()\n",
    "def create_train_test(train_dataset_filter,test_dataset_filter):\n",
    "    train_graphs,test_graphs, train_y, test_y = [],[],[],[]\n",
    "    for k,v in train_dataset_filter.items():\n",
    "        for g in v:\n",
    "            train_graphs.append(g)\n",
    "            train_y.append(label_mapping.get(k))\n",
    "    for k,v in test_dataset_filter.items():\n",
    "        for g in v:\n",
    "            test_graphs.append(g)\n",
    "            test_y.append(label_mapping.get(k))\n",
    "    return train_graphs,test_graphs, train_y, test_y\n",
    "\n",
    "def write_dataset_dic(file, train_graphs,test_graphs, train_y, test_y):\n",
    "    data_dic = {'train_x':train_graphs, 'train_y':train_y, 'test_x':test_graphs, 'test_y':test_y}\n",
    "    with open(file, 'wb') as f:\n",
    "        pickle.dump(data_dic, f)\n",
    "        f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### construct dataset for component classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ltspice-examples\n",
    "path = \"data/spice-datasets/ltspice_examples/\"\n",
    "min_limit, max_limit,instances_limit = 5,500,300\n",
    "train_data, test_data = load_graphs(path)\n",
    "train_dataset, test_dataset = construct_dataset(train_data, test_data,min_limit, max_limit)\n",
    "train_dataset_filter, test_dataset_filter = filter_data(train_dataset, test_dataset,instances_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### save dataset\n",
    "map_path = \"data/ltspice_examples_label_mapping_.pkl\"\n",
    "label_mapping = map_labels(train_dataset_filter)\n",
    "print_train_test_type_count(train_dataset_filter,test_dataset_filter)\n",
    "write_mapping(map_path, label_mapping)\n",
    "train_graphs,test_graphs, train_y, test_y = create_train_test(train_dataset_filter,test_dataset_filter)\n",
    "file_write = \"data/ltspice_examples_GC_complete.pkl\"\n",
    "write_dataset_dic(file_write, train_graphs,test_graphs, train_y, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data),len(test_data),len(train_graphs),len(test_graphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create dataset for link prediction task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset,components = create_datasets_original_circuits(train_data, test_data,min_limit, max_limit)\n",
    "map_path = \"data/ltspice_examples_label_mapping.pkl\"\n",
    "label_mapping = map_labels(train_dataset_filter)\n",
    "data_dic = {'train_x':train_dataset,  'test_x':test_dataset}\n",
    "write_mapping(map_path, components)\n",
    "file_write = \"data/ltspice_examples_LP_complete.pkl\"\n",
    "with open(file_write, 'wb') as f:\n",
    "    pickle.dump(data_dic, f)\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
