{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os.path as osp\n",
    "from itertools import chain\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from scipy.sparse.csgraph import shortest_path\n",
    "from sklearn.metrics import roc_auc_score,accuracy_score\n",
    "from torch.nn import BCEWithLogitsLoss, Conv1d, MaxPool1d, ModuleList\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import MLP, GCNConv, global_sort_pool\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "from torch_geometric.utils import k_hop_subgraph, to_scipy_sparse_matrix, from_networkx,to_networkx\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import networkx as nx\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyOwnDataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None, pre_filter=None):\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "#     @property\n",
    "#     def raw_file_names(self):\n",
    "#         return ['ltspice_examples_torch.pt']\n",
    "\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['ltspice_examples_dataset_complete.pt']\n",
    "\n",
    "#     def download(self):\n",
    "#         # Download to `self.raw_dir`.\n",
    "#         download_url(url, self.raw_dir)\n",
    "#         ...\n",
    "\n",
    "    def process(self):\n",
    "        # Read data into huge `Data` list.\n",
    "#         data_list = [...]\n",
    "        print(\"processing data now!\")\n",
    "        data_list = [torch.load('data/ltspice_examples_torch_LP_complete.pt')]\n",
    "        if self.pre_filter is not None:\n",
    "            data_list = [data for data in data_list if self.pre_filter(data)]\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data_list = [self.pre_transform(data) for data in data_list]\n",
    "\n",
    "        data, slices = self.collate(data_list)\n",
    "        print(\"saving path:\",self.processed_paths[0])\n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEALDataset(InMemoryDataset):\n",
    "    def __init__(self, dataset, num_hops, split='train'):\n",
    "        self.data = dataset[0]\n",
    "        self.num_hops = num_hops\n",
    "        super().__init__(dataset.root)\n",
    "        index = ['train', 'val', 'test'].index(split)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[index])\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['SEAL_train_data_ltspice_examples_complete.pt', 'SEAL_val_data_lstspice_examples_complete.pt', 'SEAL_test_data_lstspice_examples_complete.pt']\n",
    "\n",
    "    def process(self):\n",
    "        transform = RandomLinkSplit(num_val=0.1, num_test=0.2,\n",
    "                                    is_undirected=True, split_labels=True)\n",
    "        train_data, val_data, test_data = transform(self.data)\n",
    "        \n",
    "        self._max_z = 0\n",
    "        # Collect a list of subgraphs for training, validation and testing:\n",
    "        train_pos_data_list = self.extract_enclosing_subgraphs(\n",
    "            train_data.edge_index, train_data.pos_edge_label_index, 1)\n",
    "        train_neg_data_list = self.extract_enclosing_subgraphs(\n",
    "            train_data.edge_index, train_data.neg_edge_label_index, 0)\n",
    "\n",
    "        val_pos_data_list = self.extract_enclosing_subgraphs(\n",
    "            val_data.edge_index, val_data.pos_edge_label_index, 1)\n",
    "        val_neg_data_list = self.extract_enclosing_subgraphs(\n",
    "            val_data.edge_index, val_data.neg_edge_label_index, 0)\n",
    "\n",
    "        test_pos_data_list = self.extract_enclosing_subgraphs(\n",
    "            test_data.edge_index, test_data.pos_edge_label_index, 1)\n",
    "        test_neg_data_list = self.extract_enclosing_subgraphs(\n",
    "            test_data.edge_index, test_data.neg_edge_label_index, 0)\n",
    "\n",
    "        # Convert node labeling to one-hot features.\n",
    "        for data in chain(train_pos_data_list, train_neg_data_list,\n",
    "                          val_pos_data_list, val_neg_data_list,\n",
    "                          test_pos_data_list, test_neg_data_list):\n",
    "            # We solely learn links from structure, dropping any node features:\n",
    "            one_hot = F.one_hot(data.z, self._max_z + 1).to(torch.float)\n",
    "            x = data.x\n",
    "            data.x = torch.cat((one_hot,x),1) \n",
    "            \n",
    "\n",
    "        torch.save(self.collate(train_pos_data_list + train_neg_data_list),\n",
    "                   self.processed_paths[0])\n",
    "        torch.save(self.collate(val_pos_data_list + val_neg_data_list),\n",
    "                   self.processed_paths[1])\n",
    "        torch.save(self.collate(test_pos_data_list + test_neg_data_list),\n",
    "                   self.processed_paths[2])\n",
    "\n",
    "    def extract_enclosing_subgraphs(self, edge_index, edge_label_index, y):\n",
    "        data_list = []\n",
    "        for src, dst in edge_label_index.t().tolist():\n",
    "            sub_nodes, sub_edge_index, mapping, _ = k_hop_subgraph(\n",
    "                [src, dst], self.num_hops, edge_index, relabel_nodes=True)\n",
    "            src, dst = mapping.tolist()\n",
    "\n",
    "            # Remove target link from the subgraph.\n",
    "            mask1 = (sub_edge_index[0] != src) | (sub_edge_index[1] != dst)\n",
    "            mask2 = (sub_edge_index[0] != dst) | (sub_edge_index[1] != src)\n",
    "            sub_edge_index = sub_edge_index[:, mask1 & mask2]\n",
    "\n",
    "            # Calculate node labeling.\n",
    "            z = self.drnl_node_labeling(sub_edge_index, src, dst,\n",
    "                                        num_nodes=sub_nodes.size(0))\n",
    "\n",
    "            data = Data(x=self.data.x[sub_nodes], z=z,\n",
    "                        edge_index=sub_edge_index, y=y)\n",
    "            data_list.append(data)\n",
    "\n",
    "        return data_list\n",
    "\n",
    "    def drnl_node_labeling(self, edge_index, src, dst, num_nodes=None):\n",
    "        # Double-radius node labeling (DRNL).\n",
    "        src, dst = (dst, src) if src > dst else (src, dst)\n",
    "        adj = to_scipy_sparse_matrix(edge_index, num_nodes=num_nodes).tocsr()\n",
    "\n",
    "        idx = list(range(src)) + list(range(src + 1, adj.shape[0]))\n",
    "        adj_wo_src = adj[idx, :][:, idx]\n",
    "\n",
    "        idx = list(range(dst)) + list(range(dst + 1, adj.shape[0]))\n",
    "        adj_wo_dst = adj[idx, :][:, idx]\n",
    "\n",
    "        dist2src = shortest_path(adj_wo_dst, directed=False, unweighted=True,\n",
    "                                 indices=src)\n",
    "        dist2src = np.insert(dist2src, dst, 0, axis=0)\n",
    "        dist2src = torch.from_numpy(dist2src)\n",
    "\n",
    "        dist2dst = shortest_path(adj_wo_src, directed=False, unweighted=True,\n",
    "                                 indices=dst - 1)\n",
    "        dist2dst = np.insert(dist2dst, src, 0, axis=0)\n",
    "        dist2dst = torch.from_numpy(dist2dst)\n",
    "\n",
    "        dist = dist2src + dist2dst\n",
    "        dist_over_2, dist_mod_2 = dist // 2, dist % 2\n",
    "\n",
    "        z = 1 + torch.min(dist2src, dist2dst)\n",
    "        z += dist_over_2 * (dist_over_2 + dist_mod_2 - 1)\n",
    "        z[src] = 1.\n",
    "        z[dst] = 1.\n",
    "        z[torch.isnan(z)] = 0.\n",
    "\n",
    "        self._max_z = max(int(z.max()), self._max_z)\n",
    "\n",
    "        return z.to(torch.long)\n",
    "\n",
    "\n",
    "# path = osp.join(osp.dirname(osp.realpath(\"data/\")), '..', 'data', 'Planetoid')\n",
    "# print(\"path,\",path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = Planetoid(\"data/\", name='Cora')\n",
    "dataset = MyOwnDataset(\"data/\")\n",
    "train_dataset = SEALDataset(dataset, num_hops=2, split='train')\n",
    "val_dataset = SEALDataset(dataset, num_hops=2, split='val')\n",
    "test_dataset = SEALDataset(dataset, num_hops=2, split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DGCNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, num_layers, GNN=GCNConv, k=0.6):\n",
    "        super().__init__()\n",
    "\n",
    "        if k < 1:  # Transform percentile to number.\n",
    "            num_nodes = sorted([data.num_nodes for data in train_dataset])\n",
    "            k = num_nodes[int(math.ceil(k * len(num_nodes))) - 1]\n",
    "            k = max(10, k)\n",
    "        self.k = int(k)\n",
    "\n",
    "        self.convs = ModuleList()\n",
    "        self.convs.append(GNN(train_dataset.num_features, hidden_channels))\n",
    "        for i in range(0, num_layers - 1):\n",
    "            self.convs.append(GNN(hidden_channels, hidden_channels))\n",
    "        self.convs.append(GNN(hidden_channels, 1))\n",
    "\n",
    "        conv1d_channels = [16, 32]\n",
    "        total_latent_dim = hidden_channels * num_layers + 1\n",
    "        conv1d_kws = [total_latent_dim, 5]\n",
    "        self.conv1 = Conv1d(1, conv1d_channels[0], conv1d_kws[0],\n",
    "                            conv1d_kws[0])\n",
    "        self.maxpool1d = MaxPool1d(2, 2)\n",
    "        self.conv2 = Conv1d(conv1d_channels[0], conv1d_channels[1],\n",
    "                            conv1d_kws[1], 1)\n",
    "        dense_dim = int((self.k - 2) / 2 + 1)\n",
    "        dense_dim = (dense_dim - conv1d_kws[1] + 1) * conv1d_channels[1]\n",
    "        self.mlp = MLP([dense_dim, 128, 1], dropout=0.5, batch_norm=False)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        xs = [x]\n",
    "        for conv in self.convs:\n",
    "            xs += [conv(xs[-1], edge_index).tanh()]\n",
    "        x = torch.cat(xs[1:], dim=-1)\n",
    "\n",
    "        # Global pooling.\n",
    "        x = global_sort_pool(x, batch, self.k)\n",
    "        \n",
    "        x = x.unsqueeze(1)  # [num_graphs, 1, k * hidden]\n",
    "        x = self.conv1(x).relu()\n",
    "        x = self.maxpool1d(x)\n",
    "        x = self.conv2(x).relu()\n",
    "        x = x.view(x.size(0), -1)  # [num_graphs, dense_dim]\n",
    "        return self.mlp(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        \n",
    "        loss = criterion(out.view(-1), data.y.to(torch.float))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) * data.num_graphs\n",
    "\n",
    "    return total_loss / len(train_dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    y_pred, y_true = [], []\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        logits = model(data.x, data.edge_index, data.batch)\n",
    "        y_pred.append(logits.view(-1).cpu())\n",
    "        y_true.append(data.y.view(-1).cpu().to(torch.float))\n",
    "\n",
    "    return roc_auc_score(torch.cat(y_true), torch.cat(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = DGCNN(hidden_channels=32, num_layers=3).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.00001)\n",
    "criterion = BCEWithLogitsLoss()\n",
    "best_val_auc = test_auc = 0\n",
    "loss_list, test_list,val_list = [],[],[]\n",
    "for epoch in range(1, 50):\n",
    "    loss = train()\n",
    "    val_auc = test(val_loader)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        test_auc = test(test_loader)\n",
    "    print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
    "          f'Test: {test_auc:.4f}')\n",
    "    loss_list.append(loss)\n",
    "    test_list.append(test_auc)\n",
    "    val_list.append(val_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics_data = {'train_loss':loss_list, 'test_AUC': test_list,'val_AUC':val_list}\n",
    "# with open(\"results/metrics_ltpsice_examples_complete.pkl\",\"wb\") as f:\n",
    "#     pickle.dump(metrics_data,f)\n",
    "# f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the trained model\n",
    "torch.save(model, \"model-save/SEAL_circuit_model-1-50-ltpsice_examples_complete.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_data():\n",
    "    with open(\"data/ltspice_examples_LP_complete.pkl\",'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "        f.close()\n",
    "    with open('data/ltspice_examples_label_mapping.pkl', 'rb') as f:\n",
    "        mapping = pickle.load(f)\n",
    "        f.close()\n",
    "    test_data = dataset['test_x'] \n",
    "    return test_data, mapping\n",
    "### create test objects one by one\n",
    "def create_test_objects(test_data,mapping):\n",
    "    all_test_graphs = []\n",
    "    for g in test_data:\n",
    "        X = []\n",
    "        for i, n in enumerate(g.nodes()):\n",
    "            feat = np.zeros((5,),dtype = np.float64)\n",
    "            node_type = g.nodes[n]['type']\n",
    "            index = mapping.get(node_type)\n",
    "            feat[index] = 1.0\n",
    "            X.append(feat)\n",
    "        g_ = nx.Graph()\n",
    "        g_.add_nodes_from(g.nodes())\n",
    "        g_.add_edges_from(g.edges())\n",
    "        d = from_networkx(g_)\n",
    "        d.x = torch.from_numpy(np.array(X))\n",
    "        all_test_graphs.append(d)\n",
    "    return all_test_graphs\n",
    "##creating one test graph\n",
    "def create_one_test_graph(test_data, mapping):\n",
    "    X = []\n",
    "    all_graphs = []\n",
    "    for g in test_data:\n",
    "        for i, n in enumerate(g.nodes()):\n",
    "            feat = np.zeros((5,),dtype = np.float64)\n",
    "            node_type = g.nodes[n]['type']\n",
    "            index = mapping.get(node_type)\n",
    "            feat[index] = 1.0\n",
    "            X.append(feat)\n",
    "        g_ = nx.Graph()\n",
    "        g_.add_nodes_from(g.nodes())\n",
    "        g_.add_edges_from(g.edges())\n",
    "        all_graphs.append(g_)\n",
    "    graph = nx.disjoint_union_all(all_graphs)\n",
    "    print(\"total number of nodes and edges:\", graph.number_of_nodes(), graph.number_of_edges())\n",
    "    test_data1 = from_networkx(graph,group_node_attrs=None,group_edge_attrs=None )\n",
    "    test_data1.x = torch.from_numpy(np.array(X)).float()\n",
    "    return test_data1\n",
    "@torch.no_grad()\n",
    "def test1(model, loader):\n",
    "    model.eval()\n",
    "    y_pred, y_true = [], []\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        logits = model(data.x, data.edge_index, data.batch)\n",
    "        y_pred.append(logits.view(-1).cpu())\n",
    "        y_true.append(data.y.view(-1).cpu().to(torch.float))\n",
    "        \n",
    "    return roc_auc_score(torch.cat(y_true), torch.cat(y_pred)),torch.cat(y_true),torch.cat(y_pred)\n",
    "#     return accuracy_score(torch.sigmoid(torch.cat(y_true)), torch.sigmoid(torch.cat(y_pred))),torch.cat(y_true),torch.cat(y_pred)\n",
    "class SEAL_graph():\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "#         self.z = 28\n",
    "#         print(\"num of nodes in the graph:\", self.data.num_nodes)\n",
    "    \n",
    "    def extract_enclosing_subgraphs(self,edge_index, edge_label_index, y):\n",
    "        data_list = []\n",
    "        z_max = index = 0\n",
    "        \n",
    "        for src, dst in edge_label_index.t().tolist():\n",
    "            try:\n",
    "                index +=1\n",
    "                sub_nodes, sub_edge_index, mapping, _ = k_hop_subgraph(\n",
    "                    [src, dst], 2, edge_index, relabel_nodes=True)\n",
    "                src, dst = mapping.tolist()\n",
    "\n",
    "                # Remove target link from the subgraph.\n",
    "                mask1 = (sub_edge_index[0] != src) | (sub_edge_index[1] != dst)\n",
    "                mask2 = (sub_edge_index[0] != dst) | (sub_edge_index[1] != src)\n",
    "                sub_edge_index = sub_edge_index[:, mask1 & mask2]\n",
    "\n",
    "                # Calculate node labeling.\n",
    "                z,_max_z = self.drnl_node_labeling(sub_edge_index, src, dst,z_max, num_nodes=sub_nodes.size(0))\n",
    "                \n",
    "                z_max = _max_z\n",
    "                data = Data(x=self.data.x[sub_nodes], z=z,\n",
    "                            edge_index=sub_edge_index, y=y[index])\n",
    "                data_list.append(data)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        return data_list\n",
    "\n",
    "    def drnl_node_labeling(self,edge_index, src, dst,z_max,num_nodes=None):\n",
    "        # Double-radius node labeling (DRNL).\n",
    "        src, dst = (dst, src) if src > dst else (src, dst)\n",
    "        adj = to_scipy_sparse_matrix(edge_index, num_nodes=num_nodes).tocsr()\n",
    "\n",
    "        idx = list(range(src)) + list(range(src + 1, adj.shape[0]))\n",
    "        adj_wo_src = adj[idx, :][:, idx]\n",
    "\n",
    "        idx = list(range(dst)) + list(range(dst + 1, adj.shape[0]))\n",
    "        adj_wo_dst = adj[idx, :][:, idx]\n",
    "\n",
    "        dist2src = shortest_path(adj_wo_dst, directed=False, unweighted=True,\n",
    "                                 indices=src)\n",
    "        dist2src = np.insert(dist2src, dst, 0, axis=0)\n",
    "        dist2src = torch.from_numpy(dist2src)\n",
    "\n",
    "        dist2dst = shortest_path(adj_wo_src, directed=False, unweighted=True,\n",
    "                                 indices=dst - 1)\n",
    "        dist2dst = np.insert(dist2dst, src, 0, axis=0)\n",
    "        dist2dst = torch.from_numpy(dist2dst)\n",
    "\n",
    "        dist = dist2src + dist2dst\n",
    "        dist_over_2, dist_mod_2 = dist // 2, dist % 2\n",
    "\n",
    "        z = 1 + torch.min(dist2src, dist2dst)\n",
    "        z += dist_over_2 * (dist_over_2 + dist_mod_2 - 1)\n",
    "        z[src] = 1.\n",
    "        z[dst] = 1.\n",
    "        z[torch.isnan(z)] = 0.\n",
    "        _max_z = max(int(z.max()), z_max) \n",
    "        return z.to(torch.long),_max_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model = torch.load(\"model-save/SEAL_circuit_model-1-50-ltpsice_examples_complete.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##testing on ltspice_examples test set\n",
    "test_data, mapping = load_test_data()\n",
    "\n",
    "\n",
    "test_graphs = create_test_objects(test_data,mapping)\n",
    "\n",
    "auc_per_graph = []\n",
    "for index in range(len(test_graphs)):\n",
    "    tst_graph = test_graphs[index]\n",
    "    tst_graph.x = tst_graph.x.to(torch.float)\n",
    "    last_node = tst_graph.num_nodes-1\n",
    "    e_index = tst_graph.edge_index.numpy()\n",
    "    indices = np.where(e_index==last_node)[1]\n",
    "    if indices.shape[0]==0:continue\n",
    "    positive_edges = e_index[:,indices]\n",
    "    e_index = np.delete(e_index,np.where(e_index==last_node)[1], axis = 1)\n",
    "    self_loop = np.array([[last_node],[last_node]])\n",
    "    e_index = np.concatenate((e_index, self_loop), axis = 1)\n",
    "    neg_edges = np.zeros((2,int(last_node-positive_edges.shape[1]/2)),dtype = int)\n",
    "    n = 0\n",
    "    for i in range(last_node):\n",
    "        if i not in positive_edges:\n",
    "            neg_edges[0,n] = i\n",
    "            neg_edges[1,n] = last_node\n",
    "            n +=1\n",
    "    ### ADDING NEGATIVE EDGES TO THE GRAPH FOR CLASSIFICATION\n",
    "    tst_graph.edge_index = torch.tensor(e_index)\n",
    "    seal_data = SEAL_graph(tst_graph)\n",
    "    pos_edge_index = np.concatenate((neg_edges,positive_edges),axis = 1)\n",
    "    labels = np.zeros((pos_edge_index.shape[1]),dtype = int)\n",
    "    labels[-int(positive_edges.shape[1]/2):] =1\n",
    "    test_pos_data_list = seal_data.extract_enclosing_subgraphs(tst_graph.edge_index, torch.tensor(pos_edge_index), torch.tensor(labels))\n",
    "    z_max = 28\n",
    "    # # Convert node labeling to one-hot features.\n",
    "    for data in test_pos_data_list:\n",
    "        data.z[data.z>z_max] = z_max\n",
    "#             data.x = F.one_hot(data.z, z_max + 1).to(torch.float)\n",
    "        one_hot = F.one_hot(data.z, z_max + 1).to(torch.float)\n",
    "        x = data.x\n",
    "        data.x = torch.cat((one_hot,x),1)\n",
    "    test_loader = DataLoader(test_pos_data_list, batch_size=1)\n",
    "    test_auc,true, pred = test1(model,test_loader)\n",
    "    auc_per_graph.append(test_auc);\n",
    "\n",
    "\n",
    "print(\"average AUC: {}\".format(np.mean(auc_per_graph)))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
